/**
 * AIæœåŠ¡å±‚ - å¤„ç†æ‰€æœ‰AIç›¸å…³çš„è°ƒç”¨å’Œç¼“å­˜
 * è§£å†³AIå“åº”è¶…æ—¶å’Œæˆæœ¬ä¼˜åŒ–é—®é¢˜
 */

import { ElMessage } from 'element-plus'
import { glmApiService } from './glmApiService.js'

class AIService {
  constructor() {
    this.cache = new Map()
    this.requestQueue = []
    this.isProcessing = false
    this.maxConcurrent = 3
    this.retryConfig = {
      maxRetries: 3,
      baseDelay: 1000,
      maxDelay: 5000
    }
    
    // æ¨¡æ‹ŸAIæ¨¡å‹åˆ‡æ¢ç­–ç•¥
    this.models = {
      gpt4: { cost: 0.06, speed: 'slow', quality: 'high' },
      gpt35: { cost: 0.002, speed: 'fast', quality: 'medium' },
      local: { cost: 0, speed: 'instant', quality: 'basic' }
    }
  }

  /**
   * æ™ºèƒ½æ¨¡å‹é€‰æ‹© - æ ¹æ®æˆæœ¬å’Œå“åº”æ—¶é—´
   */
  selectModel(prompt, complexity = 'medium') {
    const cacheKey = this.generateCacheKey(prompt)
    
    // ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜
    if (this.cache.has(cacheKey)) {
      return { model: 'local', cached: true, data: this.cache.get(cacheKey) }
    }

    // æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
    if (complexity === 'high' || prompt.length > 1000) {
      return { model: 'gpt4', cached: false }
    } else if (complexity === 'medium') {
      return { model: 'gpt35', cached: false }
    } else {
      return { model: 'gpt35', cached: false }
    }
  }

  /**
   * çƒ­ç‚¹é¢„æµ‹ç®—æ³• - åŸºäºGLM AIåˆ†æå’Œå†å²æ•°æ®
   */
  async predictHotTopics(keywords, days = 7) {
    try {
      // ä¼˜å…ˆä½¿ç”¨GLM AIè¿›è¡Œçƒ­ç‚¹åˆ†æ
      if (glmApiService.isConfigured()) {
        const aiAnalysis = await glmApiService.analyzeHotTopics(keywords)
        const predictionData = await this.enhancePredictionWithAI(aiAnalysis, keywords, days)
        return {
          success: true,
          data: predictionData,
          confidence: this.calculateConfidence(predictionData),
          timestamp: new Date().toISOString(),
          source: 'ai_enhanced'
        }
      } else {
        // é™çº§åˆ°æœ¬åœ°ç®—æ³•
        const predictionData = await this.generatePrediction(keywords, days)
        return {
          success: true,
          data: predictionData,
          confidence: this.calculateConfidence(predictionData),
          timestamp: new Date().toISOString(),
          source: 'local_algorithm'
        }
      }
    } catch (error) {
      console.warn('çƒ­ç‚¹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ:', error.message)
      return {
        success: false,
        error: error.message,
        fallback: this.getFallbackPrediction(keywords)
      }
    }
  }

  /**
   * é˜Ÿåˆ—åŒ–å¤„ç† - é¿å…å¹¶å‘è¿‡å¤šå¯¼è‡´APIé™åˆ¶
   */
  async queueRequest(requestFn, priority = 'normal') {
    return new Promise((resolve, reject) => {
      this.requestQueue.push({
        fn: requestFn,
        priority,
        resolve,
        reject,
        timestamp: Date.now()
      })
      
      this.processQueue()
    })
  }

  async processQueue() {
    if (this.isProcessing || this.requestQueue.length === 0) return
    
    this.isProcessing = true
    
    // æŒ‰ä¼˜å…ˆçº§æ’åº
    this.requestQueue.sort((a, b) => {
      const priorityMap = { high: 3, normal: 2, low: 1 }
      return priorityMap[b.priority] - priorityMap[a.priority]
    })

    const activeRequests = []
    while (this.requestQueue.length > 0 && activeRequests.length < this.maxConcurrent) {
      const request = this.requestQueue.shift()
      activeRequests.push(this.executeWithRetry(request))
    }

    await Promise.allSettled(activeRequests)
    this.isProcessing = false
    
    // ç»§ç»­å¤„ç†å‰©ä½™è¯·æ±‚
    if (this.requestQueue.length > 0) {
      setTimeout(() => this.processQueue(), 100)
    }
  }

  /**
   * å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚æ‰§è¡Œ
   */
  async executeWithRetry(request) {
    let lastError
    
    for (let attempt = 0; attempt <= this.retryConfig.maxRetries; attempt++) {
      try {
        const result = await request.fn()
        request.resolve(result)
        return
      } catch (error) {
        lastError = error
        
        if (attempt < this.retryConfig.maxRetries) {
          const delay = Math.min(
            this.retryConfig.baseDelay * Math.pow(2, attempt),
            this.retryConfig.maxDelay
          )
          await new Promise(resolve => setTimeout(resolve, delay))
        }
      }
    }
    
    request.reject(lastError)
  }

  /**
   * ä½¿ç”¨AIå¢å¼ºé¢„æµ‹æ•°æ®
   */
  async enhancePredictionWithAI(aiAnalysis, keywords, days) {
    const baseTrends = await this.generatePrediction(keywords, days)
    
    // å¦‚æœAIè¿”å›äº†ç»“æ„åŒ–æ•°æ®ï¼Œä½¿ç”¨AIçš„åˆ†æç»“æœ
    if (aiAnalysis.score && aiAnalysis.trend) {
      return baseTrends.map((trend, index) => ({
        ...trend,
        score: Math.min(100, aiAnalysis.score + (Math.random() - 0.5) * 10),
        aiInsights: aiAnalysis.analysis || aiAnalysis.recommendations,
        confidence: aiAnalysis.confidence || 85
      }))
    }
    
    return baseTrends
  }

  /**
   * ç”Ÿæˆçƒ­ç‚¹é¢„æµ‹æ•°æ®
   */
  async generatePrediction(keywords, days) {
    // æ¨¡æ‹ŸåŸºäºå†å²æ•°æ®çš„é¢„æµ‹ç®—æ³•
    const baseTrends = [
      { date: new Date(), score: 75, volume: 10000, trend: 'up' },
      { date: new Date(Date.now() + 86400000), score: 82, volume: 15000, trend: 'up' },
      { date: new Date(Date.now() + 86400000 * 2), score: 91, volume: 25000, trend: 'up' },
      { date: new Date(Date.now() + 86400000 * 3), score: 88, volume: 22000, trend: 'down' },
      { date: new Date(Date.now() + 86400000 * 4), score: 79, volume: 18000, trend: 'down' },
      { date: new Date(Date.now() + 86400000 * 5), score: 85, volume: 20000, trend: 'up' },
      { date: new Date(Date.now() + 86400000 * 6), score: 93, volume: 30000, trend: 'up' }
    ]

    // æ ¹æ®å…³é”®è¯è°ƒæ•´é¢„æµ‹
    const keywordMultiplier = this.calculateKeywordMultiplier(keywords)
    const adjustedTrends = baseTrends.map(trend => ({
      ...trend,
      score: Math.min(100, trend.score * keywordMultiplier),
      volume: Math.floor(trend.volume * keywordMultiplier),
      keywords: keywords.split(' ').slice(0, 3),
      platforms: this.getPlatformRecommendations(trend.score)
    }))

    return adjustedTrends
  }

  /**
   * å…³é”®è¯å½±å“åŠ›è®¡ç®—
   */
  calculateKeywordMultiplier(keywords) {
    const keywordWeights = {
      'ai': 1.5, 'chatgpt': 1.8, 'æ•™ç¨‹': 1.3, 'è¯„æµ‹': 1.4, 'æ”»ç•¥': 1.2,
      'çƒ­ç‚¹': 1.6, 'çˆ†æ¬¾': 1.7, 'æŠ–éŸ³': 1.4, 'å°çº¢ä¹¦': 1.3, 'bç«™': 1.2
    }
    
    const words = keywords.toLowerCase().split(/\s+/)
    let multiplier = 1
    
    words.forEach(word => {
      if (keywordWeights[word]) {
        multiplier *= keywordWeights[word]
      }
    })
    
    return Math.min(multiplier, 2.5)
  }

  /**
   * å¹³å°æ¨èç®—æ³•
   */
  getPlatformRecommendations(score) {
    const platformMap = {
      90: ['æŠ–éŸ³', 'å°çº¢ä¹¦', 'Bç«™', 'å¿«æ‰‹'],
      80: ['æŠ–éŸ³', 'å°çº¢ä¹¦', 'Bç«™'],
      70: ['æŠ–éŸ³', 'å°çº¢ä¹¦'],
      60: ['å°çº¢ä¹¦', 'å¾®åš']
    }
    
    const threshold = Object.keys(platformMap).reverse().find(t => score >= parseInt(t))
    return platformMap[threshold] || ['å°çº¢ä¹¦']
  }

  /**
   * ç½®ä¿¡åº¦è®¡ç®—
   */
  calculateConfidence(predictionData) {
    const volatility = this.calculateVolatility(predictionData)
    const baseConfidence = 85
    return Math.max(60, baseConfidence - volatility * 10)
  }

  calculateVolatility(data) {
    const scores = data.map(d => d.score)
    const mean = scores.reduce((a, b) => a + b, 0) / scores.length
    const variance = scores.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / scores.length
    return Math.sqrt(variance) / 10
  }

  /**
   * ç¼“å­˜ç®¡ç†
   */
  generateCacheKey(prompt) {
    return btoa(prompt.toLowerCase().trim()).substring(0, 16)
  }

  setCache(key, data, ttl = 3600000) { // 1å°æ—¶TTL
    this.cache.set(key, {
      data,
      expiry: Date.now() + ttl
    })
    
    // æ¸…ç†è¿‡æœŸç¼“å­˜
    this.cleanupCache()
  }

  cleanupCache() {
    const now = Date.now()
    for (const [key, value] of this.cache.entries()) {
      if (value.expiry < now) {
        this.cache.delete(key)
      }
    }
  }

  /**
   * é™çº§é¢„æµ‹æ–¹æ¡ˆ
   */
  getFallbackPrediction(keywords) {
    return [
      {
        date: new Date(),
        score: 70,
        volume: 5000,
        trend: 'stable',
        keywords: keywords.split(' ').slice(0, 2),
        platforms: ['å°çº¢ä¹¦', 'æŠ–éŸ³'],
        isFallback: true
      }
    ]
  }

  /**
   * AIå†…å®¹ç”Ÿæˆ - ä½¿ç”¨GLM-4-Flash
   */
  async generateContent(prompt, type = 'article', options = {}) {
    try {
      if (glmApiService.isConfigured()) {
        const result = await glmApiService.generateContent(prompt, type, options)
        return {
          success: true,
          content: result.content,
          usage: result.usage,
          model: result.model,
          timestamp: result.timestamp
        }
      } else {
        // é™çº§åˆ°æ¨¡æ‹Ÿå†…å®¹
        return {
          success: true,
          content: this.generateMockContent(prompt, type),
          model: 'mock',
          timestamp: new Date().toISOString()
        }
      }
    } catch (error) {
      console.warn('AIå†…å®¹ç”Ÿæˆå¤±è´¥:', error.message)
      return {
        success: false,
        error: error.message,
        fallback: this.generateMockContent(prompt, type)
      }
    }
  }

  /**
   * æ™ºèƒ½æ¨è - åŸºäºç”¨æˆ·ç”»åƒ
   */
  async getSmartRecommendations(userProfile, contentType = 'mixed') {
    try {
      if (glmApiService.isConfigured()) {
        const recommendations = await glmApiService.getRecommendations(userProfile, contentType)
        return {
          success: true,
          data: recommendations,
          timestamp: new Date().toISOString(),
          source: 'ai_powered'
        }
      } else {
        return {
          success: true,
          data: this.getMockRecommendations(userProfile, contentType),
          timestamp: new Date().toISOString(),
          source: 'rule_based'
        }
      }
    } catch (error) {
      console.warn('æ™ºèƒ½æ¨èå¤±è´¥:', error.message)
      return {
        success: false,
        error: error.message,
        fallback: this.getMockRecommendations(userProfile, contentType)
      }
    }
  }

  /**
   * å†…å®¹ä¼˜åŒ–å»ºè®®
   */
  async optimizeContent(content, platform, goal = 'engagement') {
    try {
      if (glmApiService.isConfigured()) {
        const optimization = await glmApiService.optimizeContent(content, platform, goal)
        return {
          success: true,
          data: optimization,
          timestamp: new Date().toISOString()
        }
      } else {
        return {
          success: true,
          data: this.getMockOptimization(content, platform, goal),
          timestamp: new Date().toISOString()
        }
      }
    } catch (error) {
      console.warn('å†…å®¹ä¼˜åŒ–å¤±è´¥:', error.message)
      return {
        success: false,
        error: error.message,
        fallback: this.getMockOptimization(content, platform, goal)
      }
    }
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿå†…å®¹ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
   */
  generateMockContent(prompt, type) {
    const templates = {
      article: `åŸºäº"${prompt}"çš„æ·±åº¦åˆ†æ\n\nè¿™æ˜¯ä¸€ä¸ªå…³äº${prompt}çš„è¯¦ç»†åˆ†ææ–‡ç« ã€‚å†…å®¹åŒ…æ‹¬èƒŒæ™¯ä»‹ç»ã€æ ¸å¿ƒè§‚ç‚¹ã€å®ä¾‹åˆ†æå’Œæ€»ç»“å»ºè®®ã€‚\n\nè¯·æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿå†…å®¹ï¼Œå»ºè®®é…ç½®GLM APIè·å¾—æ›´å¥½çš„æ•ˆæœã€‚`,
      social: `ğŸ”¥ ${prompt} çƒ­ç‚¹è§£æ\n\nâœ¨ æ ¸å¿ƒè¦ç‚¹\nğŸ“ˆ è¶‹åŠ¿åˆ†æ\nğŸ’¡ å®ç”¨å»ºè®®\n\n#${prompt.replace(/\s+/g, '')} #çƒ­ç‚¹åˆ†æ`,
      marketing: `${prompt} - ä¸å®¹é”™è¿‡çš„æœºä¼šï¼\n\nğŸ¯ æ ¸å¿ƒä¼˜åŠ¿\nğŸ’° è¶…å€¼ä½“éªŒ\nâ° é™æ—¶ä¼˜æƒ \n\nç«‹å³è¡ŒåŠ¨ï¼ŒæŠŠæ¡å…ˆæœºï¼`,
      tutorial: `${prompt} å®Œæ•´æ•™ç¨‹\n\nğŸ“‹ å‡†å¤‡å·¥ä½œ\nğŸ”§ æ“ä½œæ­¥éª¤\nâœ… éªŒè¯ç»“æœ\nğŸ’¡ è¿›é˜¶æŠ€å·§\n\nè·Ÿç€æ­¥éª¤ï¼Œè½»æ¾æŒæ¡ï¼`
    }
    
    return templates[type] || templates.article
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿæ¨èï¼ˆé™çº§æ–¹æ¡ˆï¼‰
   */
  getMockRecommendations(userProfile, contentType) {
    return {
      topics: ['AIæŠ€æœ¯è¶‹åŠ¿', 'å†…å®¹åˆ›ä½œæŠ€å·§', 'ç¤¾äº¤åª’ä½“è¿è¥', 'æ•°æ®åˆ†ææ–¹æ³•', 'ç”¨æˆ·ä½“éªŒè®¾è®¡'],
      styles: ['ä¸“ä¸šåˆ†æ', 'è½»æ¾å¹½é»˜', 'å®ç”¨æ•™ç¨‹', 'æ·±åº¦è§£è¯»', 'çƒ­ç‚¹è¿½è¸ª'],
      timing: ['09:00-11:00', '14:00-16:00', '19:00-21:00'],
      platforms: ['å°çº¢ä¹¦', 'æŠ–éŸ³', 'Bç«™', 'å¾®åš'],
      confidence: 75
    }
  }

  /**
   * ç”Ÿæˆæ¨¡æ‹Ÿä¼˜åŒ–å»ºè®®ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
   */
  getMockOptimization(content, platform, goal) {
    return {
      optimized_content: content + '\n\n[AIä¼˜åŒ–å»ºè®®ï¼šæ·»åŠ æ›´å¤šäº’åŠ¨å…ƒç´ å’Œçƒ­é—¨æ ‡ç­¾]',
      suggestions: [
        'å¢åŠ emojiè¡¨æƒ…æå‡è§†è§‰æ•ˆæœ',
        'æ·»åŠ ç›¸å…³çƒ­é—¨æ ‡ç­¾',
        'ä¼˜åŒ–å¼€å¤´å¸å¼•æ³¨æ„åŠ›',
        'å¢åŠ äº’åŠ¨æ€§é—®é¢˜'
      ],
      expected_improvement: 'é¢„è®¡äº’åŠ¨ç‡æå‡15-25%'
    }
  }

  /**
   * æ‰¹é‡AIå¤„ç†ä¼˜åŒ–
   */
  async batchProcess(items, processFn) {
    const batchSize = 10
    const results = []
    
    for (let i = 0; i < items.length; i += batchSize) {
      const batch = items.slice(i, i + batchSize)
      const batchResults = await Promise.allSettled(
        batch.map(item => this.queueRequest(() => processFn(item)))
      )
      
      results.push(...batchResults.map((result, index) => ({
        item: batch[index],
        success: result.status === 'fulfilled',
        data: result.status === 'fulfilled' ? result.value : null,
        error: result.status === 'rejected' ? result.reason : null
      })))
    }
    
    return results
  }
}

// åˆ›å»ºå…¨å±€å®ä¾‹
export const aiService = new AIService()
export default AIService